{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough: Scoring a trained CNTK model with PySpark on a Microsoft Azure HDInsight cluster\n",
    "\n",
    "\n",
    "This notebook demonstrates how a trained [Microsoft Cognitive Toolkit](https://github.com/Microsoft/CNTK/wiki) deep learning model can be applied to files in a distributed and scalable fashion using the [Spark Python API](http://spark.apache.org/docs/2.1.0/programming-guide.html) (PySpark). An image classification model pretrained on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset is applied to 10,000 withheld images. A sample of the images is shown below along with their classes:\n",
    "\n",
    "<img src=\"https://cntk.ai/jup/201/cifar-10.png\" width=500 height=500>\n",
    "\n",
    "To begin, follow the instructions below to set up a cluster and storage account. You will be prompted to upload a copy of this notebook to the cluster, where you can continue following the walkthrough by executing the PySpark code cells.\n",
    "\n",
    "### Outline\n",
    "- [Load sample images into a Spark Resiliant Distributed Dataset or RDD](#images)\n",
    "   - [Load modules and define presets](#imports)\n",
    "   - [Download the dataset locally on the Spark cluster](#tarball)\n",
    "   - [Convert the dataset into an RDD](#rdd)   \n",
    "- [Score the images using a trained CNTK model](#score)\n",
    "   - [Download the trained CNTK model to the Spark cluster](#model)\n",
    "   - [Define functions to be used by worker nodes](#functions)\n",
    "   - [Score the images on worker nodes](#map)\n",
    "   - [Evaluate model accuracy](#evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"images\"></a>\n",
    "## Load sample images into a Spark Resiliant Distributed Dataset or RDD\n",
    "\n",
    "We will now use Python to obtain the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image set compiled and distributed by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. For more details on the dataset, see Alex Krizhevsky's [Learning Multiple Layers of Features from Tiny Images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf) (2009).\n",
    "\n",
    "<a name=\"imports\"></a>\n",
    "### Load modules and define presets\n",
    "\n",
    "Execute the cell below by selecting it with the mouse or arrow keys, then pressing Shift+Enter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cntk import load_model\n",
    "import findspark\n",
    "findspark.init('/root/spark-2.1.0-bin-hadoop2.6')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc =SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "import tarfile\n",
    "from urllib.request import urlretrieve\n",
    "import xml.etree.ElementTree\n",
    "\n",
    "cifar_uri = 'http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz' # Location of test image dataset\n",
    "mean_image_uri = 'https://raw.githubusercontent.com/Azure-Samples/hdinsight-pyspark-cntk-integration/master/CIFAR-10_mean.xml' # Mean image for subtraction\n",
    "model_uri = 'https://github.com/Azure-Samples/hdinsight-pyspark-cntk-integration/raw/master/resnet20_meanimage_159.dnn' # Location of trained model\n",
    "local_tmp_dir = '/tmp/cifar'\n",
    "local_cifar_path = os.path.join(local_tmp_dir, os.path.basename(cifar_uri))\n",
    "local_model_path = os.path.join(local_tmp_dir, 'model.dnn')\n",
    "local_mean_image_path = os.path.join(local_tmp_dir, 'mean_image.xml')\n",
    "os.makedirs(local_tmp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"tarball\"></a>\n",
    "### Download the dataset locally on the Spark cluster\n",
    "\n",
    "The image data are `ndarray`s stored in a Python `dict` which has been pickled and tarballed. The cell below downloads the tarball and extracts the `dict` containing the test image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(local_cifar_path):\n",
    "    urlretrieve(cifar_uri, filename=local_cifar_path)\n",
    "\n",
    "with tarfile.open(local_cifar_path, 'r:gz') as f:\n",
    "    test_dict = pickle.load(f.extractfile('cifar-10-batches-py/test_batch'), encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rdd\"></a>\n",
    "### Convert the dataset into an RDD\n",
    "\n",
    "The following code cell illustrates how the collection of images can be distributed to create a Spark RDD. The cell creates an RDD with one partition per worker to limit the number of times that the trained model must be reloaded during scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape_image(record):\n",
    "    image, label, filename = record\n",
    "    return image.reshape(3,32,32).transpose(1,2,0), label, filename\n",
    "\n",
    "image_rdd = sc.parallelize(zip(test_dict['data'], test_dict['labels'], test_dict['filenames']))\n",
    "image_rdd = image_rdd.map(reshape_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convince ourselves that the data has been properly loaded, let's visualize a few of these images. For plotting, we will need to transfer them to the local context by way of a Spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_images = image_rdd.take(5)\n",
    "image_data = np.array([i[0].reshape((32*32*3)) for i in sample_images]).T\n",
    "image_labels = [i[2] for i in sample_images]\n",
    "image_df = pd.DataFrame(image_data, columns=image_labels)\n",
    "spark.createDataFrame(image_df).coalesce(1).write.mode(\"overwrite\").csv(\"/tmp/cifar_image\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "image_df = pd.read_csv(glob('/tmp/cifar_image/*.csv')[0])\n",
    "plt.figure(figsize=(15,1))\n",
    "for i, col in enumerate(image_df.columns):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    image = image_df[col].values.reshape((32, 32, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.title(col)\n",
    "    cur_axes = plt.gca()\n",
    "    cur_axes.axes.get_xaxis().set_visible(False)\n",
    "    cur_axes.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"score\"></a>\n",
    "## Score the images using a trained CNTK model\n",
    "\n",
    "Now that the cluster and sample dataset have been created, we can use PySpark to apply a trained model to the images.\n",
    "\n",
    "<a name=\"model\"></a>\n",
    "### Download the trained CNTK model and mean image to the Spark cluster\n",
    "\n",
    "We previously trained a twenty-layer ResNet model to classify CIFAR-10 images by following [this tutorial](https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Classification/ResNet) from the CNTK git repo. The model expects input images to be preprocessed by subtracting the mean image defined in an OpenCV XML file. The following cell downloads both the trained model and the mean image, and ensures that data from both files can be accessed by worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urlretrieve(model_uri, local_model_path)\n",
    "sc.addFile(local_model_path)\n",
    "\n",
    "urlretrieve(mean_image_uri, local_mean_image_path)\n",
    "mean_image = xml.etree.ElementTree.parse(local_mean_image_path).getroot()\n",
    "mean_image = [float(i) for i in mean_image.find('MeanImg').find('data').text.strip().split(' ')]\n",
    "mean_image = np.array(mean_image).reshape((32, 32, 3)).transpose((2, 0, 1))\n",
    "mean_image_bc = sc.broadcast(mean_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"functions\"></a>\n",
    "### Define functions to be used by worker nodes\n",
    "\n",
    "The following functions will be used during scoring to load, preprocess, and score images. A class label (integer in the range 0-9) will be returned for each image, along with its filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_image(my_image, mean_image):\n",
    "    ''' Reshape and flip RGB order '''\n",
    "    my_image = my_image.astype(np.float32)\n",
    "    bgr_image = my_image[:, :, ::-1] # RGB -> BGR\n",
    "    image_data = np.ascontiguousarray(np.transpose(bgr_image, (2, 0, 1)))\n",
    "    image_data -= mean_image\n",
    "    return(image_data)\n",
    "\n",
    "def run_worker(records):\n",
    "    ''' Scoring script run by each worker '''\n",
    "    loaded_model = load_model(SparkFiles.get('./model.dnn'))\n",
    "    mean_image = mean_image_bc.value\n",
    "\n",
    "    # Iterate through the records in the RDD.\n",
    "    # record[0] is the image data\n",
    "    # record[1] is the true label\n",
    "    # record[2] is the file name\n",
    "    for record in records:\n",
    "        preprocessed_image = get_preprocessed_image(record[0], mean_image)\n",
    "        dnn_output = loaded_model.eval({loaded_model.arguments[0]: [preprocessed_image]})\n",
    "        yield record[1], np.argmax(np.squeeze(dnn_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a name=\"map\"></a>\n",
    "### Score the images on worker nodes\n",
    "\n",
    "The code cell below maps each partition of `image_rdd` to a worker node and collects the results. Runtimes of 1-3 minutes are typical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_images = image_rdd.mapPartitions(run_worker)\n",
    "\n",
    "# Time how long it takes to score 10k test images\n",
    "start = pd.datetime.now()\n",
    "results = labelled_images.collect()\n",
    "print('Scored {} images'.format(len(results)))\n",
    "stop = pd.datetime.now()\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"evaluate\"></a>\n",
    "### Evaluate model accuracy\n",
    "\n",
    "The trained model assigns a class label (represented by an integer value 0-9) to each image. We now compare the true and predicted class labels to evaluate our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results, columns=['true_label', 'predicted_label'])\n",
    "\n",
    "num_correct = sum(df['true_label'] == df['predicted_label'])\n",
    "num_total = len(results)\n",
    "print('Correctly predicted {} of {} images ({:0.2f}%)'.format(num_correct, num_total, 100 * num_correct / num_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct a confusion matrix to visualize which classification errors are most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(df).coalesce(1).write.mode(\"overwrite\").csv(\"/tmp/cifar_scores\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "df = pd.read_csv(glob('/tmp/cifar_scores/*.csv')[0])\n",
    "print('Constructing a confusion matrix with the first {} samples'.format(len(df.index)))\n",
    "\n",
    "label_to_name_dict = {0: 'airplane',\n",
    "                      1: 'automobile',\n",
    "                      2: 'bird',\n",
    "                      3: 'cat',\n",
    "                      4: 'deer',\n",
    "                      5: 'dog',\n",
    "                      6: 'frog',\n",
    "                      7: 'horse',\n",
    "                      8: 'ship',\n",
    "                      9: 'truck'}\n",
    "\n",
    "labels = np.sort(df['true_label'].unique())\n",
    "named_labels = [label_to_name_dict[i] for i in labels]\n",
    "cm = confusion_matrix(df['true_label'], df['predicted_label'], labels=labels)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(labels))\n",
    "plt.xticks(tick_marks, named_labels, rotation=90)\n",
    "plt.yticks(tick_marks, named_labels)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix shows that the most common errors include:\n",
    "- mislabeling cats as dogs, and vice versa\n",
    "- mislabeling birds as airplanes, and vice versa\n",
    "- confusing \"automobiles\" with \"trucks\", and vice versa"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
